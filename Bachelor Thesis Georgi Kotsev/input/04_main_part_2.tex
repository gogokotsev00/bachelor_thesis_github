\chapter{Implementation} \label{implementation}
This chapter describes how the approach we proposed in Chapter \ref{approach} is applied by using the Python API offered by CARLA. Moreover, we reveal the software implementation of the processes behind each of the three stages in our simulation pipeline.  

\section{Setup of Virtual Environment}
The simulation pipeline consists of three main stages - Data Generation for the experiments, Simulation of possible real-world scenarios and Evaluation of the results, as it was explained in the previous chapter. In the course of our first stage, we connect the client to the simulator's server, which represents the digital world for our research. The 3D simulator provides two options for a communication between client and server\footnote{\url{https://carla.readthedocs.io/en/latest/adv_synchrony_timestep/}, visited on 02/12/2022}:
\begin{itemize}
    \item \textbf{synchronous mode} - in this mode the client must notify via tick when all the commands are prepared that the server is obliged to execute. This is the mode which is relevant for simulations where synchrony between, i.e., sensors and world is crucial. 
    \item \textbf{asynchronous mode} - this is the default mode, where the server runs the simulation as fast as possible and does not have to wait for a signal from the client, which ensures that everything is ready for the next step 
\end{itemize}

For this work the synchronous mode is set, because we require an environment, where we know location and rotation of world objects, receive accurate data from the camera and maintain a consistent connection to the server. When we enable the synchronous mode, we create a synchronous instance of the Traffic Manager, which handles any group of vehicles in the simulated world.

\subsection{Sensor placement}
Having all synchrony-related settings adjusted, we then instantiate a new object of type Sensor from CARLA's blueprint library\footnote{\url{https://carla.readthedocs.io/en/latest/bp_library/}, visited on 03/12/2022}. This is a subtype of actor which can measure and stream data. For this thesis the sensor is instance segmentation camera which has the main parameters of each camera sensor: field of view, width and height of generated image. What is more, a sensor is responsible for perceiving data either whenever a new event is registered or on each tick signal received from the client. This sensor in particular has a task to render all elements contained in the field of view with a specific colour according to their semantic tags (vehicle, building, traffic light, etc.) and a unique object ID (assigned by the Unreal Engine at the moment of instantiation). 

With regard to the spawning of sensor, in CARLA one can attach it to a vehicle and thus to visualise an on-board point of view. However, the goal of this thesis is to find an optimal position in the infrastructure, therefore we spawn our traffic monitoring tool as displayed in Figure \ref{fig:camera_positions} on particular locations. Again, there is no algorithm for locations generation, they are completely random and aim to compare three points in each border of the region of interest, which makes eight in total. In order to receive data, which is then forwarded along the pipeline for examination, sensors have an integrated listening method. In short, there we provide as an argument a function that inserts every new image in a queue, which then can be accessed during the evaluation part. At the end, we define the height and tilt angle of the camera so that it points to the intersection and have an unobstructed view. More information about what values for the camera attributes are set for the experiments is described in chapter \ref{evaluation}.

\subsection{Waypoints generation}
The last step from the first stage is generation of waypoints suitable for our experiments, where pairs of vehicles are going to be placed to recreate real-world situations and estimate occlusion percentage, because it prevents the camera from a correct perception of the surrounding environment. CARLA supports methods\footnote{\url{https://carla.readthedocs.io/en/latest/core_map/}, visited on 03/12/2022}, which allow a user to select a landmark on the map and spawn an object there. In order to extract these points from the simulator, we have to specify which area is of interest. As we previously mentioned, we do not consider cases where a vehicle is spawned on impermissible places like pavements or green spaces. Therefore, the task of this work requires only valid road lanes as spawn location. Moreover, the simulated world offers the opportunity to choose from points which belong to possible driving routes and the process is the following one:
\begin{enumerate}
    \item The user gives a location as input and the simulator returns the nearest waypoint to it that lies on the premises of our region of interest, which in our case is an intersection of four roads.
    \item Knowing this waypoint, one can request from the simulator to return the junction which contains it.
    \item The returned junction comes with a list of various waypoints, therefore it should be specified in a method for extraction that we only need positions on driving lanes.
    \item In the previous step, we receive a list of pairs of points, which mark the beginning and the end of a lane. With a for-loop we iterate through all pairs and use a method from the simulator that returns a list of waypoints in the direction of the line a distance D apart. The new list contains all points from the current waypoint to the end of its lane.
\end{enumerate}
It is beneficial for the needs of our experiments to be able to define what distance should exist between the spawn positions. To illustrate an exemplary situation we use Figure \ref{fig:distance_waypoints}, where the distance between each point in a single lane is set to five meters. When the preparation of vehicle spawn locations is done, the simulation is started, which is explained in the following section.

\begin{figure} [h!]
    \centering
    \includegraphics[width=0.75\textwidth]{images/waypoints_target.png}
    \caption[Vehicle waypoints with specified distance]{This image serves as an example about where we could place vehicles when the distance between the positions in each lane is 5 meters.}
    \label{fig:distance_waypoints}
\end{figure}

\section{Simulation of valid scenarios}
For the purpose of suggesting an optimal position for a camera sensor in the infrastructure that could enhance autonomous vehicles, improve communication between them and facilitate mobility per se, we have to reproduce real-world scenarios and observe how the sensor perform. The focus of this work is the occlusion, which occurs in everyday traffic when larger vehicles intercept a sensor's field of view, therefore we apply our own approach based on the Occlusion Degree Model proposed by Du et al. in \cite{occlusion_degree_model}. In subsection \ref{sec:sim_stage} we give a detailed explanation of how our experiments are designed, whereas here the technical means used by this stage are addressed.

\subsection{Spawning vehicles and their roles}
To start with, in each simulation we have two actor objects from type Carla.Vehicle, which can be chosen from the blueprint library\footnote{\url{https://carla.readthedocs.io/en/latest/bp_library/\#vehicle}, visited on 03/12/2022}. In this library a user can select whichever vehicle he needs for his purposes. In addition, vehicles are varying from motorcycles through small cars to large trucks. Again, our implementation inserts two vehicles in the virtual environment, which take the roles 'occluder' and 'target' depending on the vehicle's size. This means that if we put the label occluder to a vehicle 2x3m-sized vehicle and as a target vehicle we have a 4x8m-sized truck, it is certain that such scenario will be useless for experimental purposes, because in 95\% of the cases a normal object recognition algorithm would not have troubles in detecting it. For this reason, we suggest that always the occluder is larger than the target vehicle, which will guarantee more reliability of the results if this approach is to be adopted by further studies.

At the beginning, we generate a list of waypoints for both vehicles with a specified distance between them. Vehicles can only be spawned at the exact location of a waypoint and in the direction of the driving lane under them, which means that it is possible to spawn our actor on the same place multiple times but with different rotation. One important functionality of these waypoints is that the vehicle is always placed parallel to the ground and could not be hovering at an angle from thirty degrees, for example. Furthermore, this guarantees flexibility when simulating scenarios, because it is not necessary to always use a flat surface for the vehicles. 

Before placing the target on the first received waypoint, it is checked if the vehicle's centre is displayed on the image plane. The process is explained in subsection \ref{camera_model} and consists of several steps:
\begin{enumerate}
    \item World to camera matrix (W2C) generation - CARLA provides a method which returns the inverse matrix of the sensor as an array.
    \item Camera's intrinsic matrix $A$ generation - it contains the parameters of the sensor like coordinates of the principal point $u_0$ and $v_0$, skew index $\gamma=0$ (no distortion) and scale factors $\alpha$ and $\beta$ which have as a value the result of the focal length divided by the pixel size.
    \item 2D Projection of a 3D world point - we take the coordinates of the world point in an array, multiply it with the W2C matrix and change from Unreal Engine's coordinate system to a standard one $(x, y ,z) \rightarrow (y, -z, x)$. Afterwards, we multiply with the new array with the intrinsic matrix $A$ and both $x$ and $y$ are normalized by the third value in the array.
    \item If the final coordinates are negative or exceeds either the image's width or height then the vehicle is not spawned as it is practically invisible to the camera, and no occlusion could happen.
\end{enumerate}

When this check returns a positive output, we spawn the target vehicle on a location which is different from the target's one, because they cannot be spawned at the exact same point. The above described process checks again whether the occluder is visible for the camera and if that is the case it is placed successfully. During each iteration the camera records the situation on the road and streams data for evaluation of the occlusion, which is explained in the next subsection. Then the occluder is destroyed and simulation continues while the list with spawn points is not empty.

 \subsection{Camera output and occlusion calculation}
 In this work is used the instance segmentation camera as a reliable sensor that provides only ground-truth data. The most important advantage of this type of camera is that it can perform better than an object recognition method. However, it is still under development by the CARLA creators and has one drawback that after we spawn the target vehicle, we have to wait several ticks until it's displayed on the image plane. This happens because in the simulator a vehicle is always spawned in the air, therefore it needs some time to land on the ground and the world's physics to be adjusted. Nevertheless, this issue does not cause any delays for the simulation.
 
 To calculate the occlusion, we need an image of the target vehicle alone and an image where both vehicles are available in the world. Therefore, the first image is taken after the target vehicle is spawned, then if the occluder can also be spawned we destroy the target and respawn it, so that it can be on the same position as in the previous image. The main reason for doing this is again the fact that vehicles fall some time before they finally land on the ground, which causes change in the number of pixels containing the vehicle and consequently the result's accuracy. For the calculation we use the following help methods:
 \begin{itemize}
     \item \textbf{Conversion of raw image date to a 2D array} - the data that is output by a camera is in a raw format, which should be transformed into a 2D array so that we could iterate through all pixels. This method extracts the blue, green and red values for each pixel and puts them in the exact row and column as the location of the pixels on the image plane. Blue and green channels define the object's unique ID, and the red one assigns it to a class according to its type.
     \item \textbf{Calculation of pixels containing a specific label} - in CARLA each item has a tag which defines the type of the object, e.g., whether it is a vehicle a building or something else. This method calculates how many pixels contain a given type using the information from the red channel. 
 \end{itemize}

With the first help method we are able to generate arrays through which we can iterate while the second one tells us how many pixels is the target vehicle, when the occluder is absent. We then iterate through the 2D array and look only in rows whose pixels contain objects from type vehicle. In each row we check again only columns containing a vehicle and calculate the ID of a vehicle using the blue and green value of a pixel. An important point to be made is that Unreal Engine gives an ID to each object spawned in the 3D world, which is not always the result of the expression \ref{eq:instance}, where B represents the blue value and G the green one.
When the ID crosses the hexadecimal value of $0\times10000$ which is 65536 in a decimal number system, we have to subtract this value from the object's ID so many times as it is contained in it, which is shown in the equation \ref{eq:new_id}.

\begin{equation}B + G * 256\label{eq:instance}\end{equation}

\begin{equation}\textrm{new\_ID} \: = \textrm{ID} - \lfloor \textrm{ID} / 65535 \rfloor * 65536\label{eq:new_id}\end{equation}

After we are done with the calculations, we check if the occluder's ID equals the converted blue and green channels' values and increment the number of target's visible pixels. If we have reached a situation where the visible numbers are equal to all pixels from the first image, meaning the vehicle is not occluded, we break the loop and return the occlusion, which comprises the result of equation \ref{eq:occlusion}. Apart from the occlusion, the location of the target vehicle is also included in a text document after each iteration. At the end of the simulation, we also add the number of times a vehicle was not visible on the camera and therefore not spawned to the document, which is crucial for the evaluation stage.  

\begin{equation}
    \frac{\textrm{target\_max\_pixels} - \textrm{target\_visible\_pixels}}{\textrm{target\_max\_pixels}} * 100 \label{eq:occlusion}
\end{equation}
 
 \section{Evaluation of the results}